#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass classicthesis
\use_default_options true
\maintain_unincluded_children false
\language brazilian
\language_package default
\inputencoding utf8
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 0
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine natbib
\cite_engine_type numerical
\biblio_style plainnat
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 2
\tocdepth 2
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Chapter
Redes neurais residuais
\end_layout

\begin_layout Section
Introdução
\end_layout

\begin_layout Standard
Redes neurais profundas são redes com mais de uma camada oculta de neurônios.
 Devido ao maior número de camadas ocultas, as redes neurais profundas são
 capazes de integrar atributos de vários níveis de abstração de maneira
 hierárquica.
 Isso ocorre porque cada camada oculta de neurônios constrói atributos derivados
 a partir da informação contida nas camadas anteriores.
\end_layout

\begin_layout Standard
Dentre as arquiteturas de redes neurais profundas, as redes neurais convoluciona
is tem obtido grande sucesso em diversas aplicações relacionadas a imagens,
 como por exemplo, a classificação e a detecção de objetos em fotografias.
 
\end_layout

\begin_layout Standard
Nas redes convolucionais são utilizados neurônios que possuem conexões locais
 com a camada imediatamente anterior.
 Cada tipo de neurônio é então aplicado a camada anterior realizando uma
 espécie de varredura.
 Assim, um tipo de neurônio que responde a um determinado padrão local,
 irá responder a esse padrão independentemente de onde ele ocorra espacialmente.
 Isso difere das redes neurais totalmente conectadas (
\emph on
Fully Connected
\emph default
), como as utilizadas no capítulo 
\begin_inset CommandInset ref
LatexCommand ref
reference "ch:rede_neural_similar_hk"

\end_inset

, onde cada neurônio de uma camada está conectado a todos os neurônios da
 camada anterior.
 
\end_layout

\begin_layout Standard
Para uma explicação mais intuitiva, podemos pensar em uma rede neural convolucio
nal cujo objetivo é identificar a ocorrência de um rosto humano em fotografias.
 Nessa rede poderíamos ter um tipo de neurônio que seria responsável por
 identificar a presença de um olho, outro tipo neurônio para um nariz e
 outro tipo para a boca.
 Cada neurônio estaria conectado a uma pequena região da imagem, mas o conjunto
 de todos os neurônios do mesmo tipo estariam distribuídos cobrindo toda
 a imagem.
 Assim, neurônios do tipo que respondem a presença da boca emitiriam uma
 resposta somente na região da imagem onde há uma boca.
 O mesmo ocorreria para os demais tipos de neurônios.
 Na camada seguinte, um tipo de neurônio capaz de identificar faces humanas
 iria sinalizar a presença da mesma caso houvessem sinais de boca, nariz
 e olhos em regiões próximas.
 Essa rede exemplifica a hierarquia entre as camadas e a identificação de
 padrões mais complexos derivados de padrões mais simples.
 Caso fosse utilizada uma rede neural totalmente conectada, mesmo utilizando
 várias camadas ocultas, seria perdida a referência espacial.
 Isto significa que mesmo que houvessem neurônios capazes de identificar
 olhos, bocas e narizes, a rede não saberia interpretar se eles estariam
 posicionados numa face ou apenas como elementos dispersos na foto.
 
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Inserir uma imagem para exemplificar
\end_layout

\end_inset


\end_layout

\begin_layout Standard
A capacidade de detectar padrões locais, combinada as múltiplas camadas
 das redes profundas, possibilita criarmos um modelo de predição com semelhanças
 importantes ao modelo de autômatos celulares.
 Essas semelhanças, as quais motivaram esse trabalho, são a identificação
 de padrões locais na sequência capazes de iniciar a formação e/ou propagarem
 essa informação ao longo da proteína até a formação da estrutura secundária
 nativa.
\end_layout

\begin_layout Standard
Quando ambos os métodos são comparados, redes neurais convolucionais profundas
 e autômatos celulares, podemos listar 
\emph on
a priori
\emph default
 algumas vantagens e desvantagens.
 Dentre as vantagens das redes neurais convolucionais profundas temos:
\end_layout

\begin_layout Itemize
o modelo não exige uma pré determinação dos estados intermediários, como
 foi necessário com os autômatos celulares, os quais necessitam de estados
 discretos;
\end_layout

\begin_layout Itemize
as redes neurais possuem métodos de treinamento bem estabelecidos, eficazes
 e eficientes.
\end_layout

\begin_layout Standard
Por outro lado, os autômatos celulares possuem a vantagem de facilitarem
 a análise e compreensão dos padrões aprendidos e de eventos de formação
 e propagação da estrutura secundária.
\end_layout

\begin_layout Standard
Entretanto, como demonstrado no capítulo anterior, a definição de um modelo
 de autômatos celulares capazes de representar a estrutura secundária é
 limitada pela dificuldade de se buscar regras que reproduzam o comportamento
 esperado.
 Ou seja, pela grande dificuldade imposta pelo problema inverso dos autômatos
 celulares.
\end_layout

\begin_layout Standard
Nesse capítulo, exploramos a utilização de redes neurais convolucionais
 profundas como um método alternativo ao uso de autômatos celulares.
 É importante notar que apesar da mudança de modelo, o objetivo continua
 ser predizer a estrutura secundária a partir da informação contida unicamente
 na sequência de aminoácidos utilizando padrões locais que se propagam a
 longo da sequência.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Deep networks naturally integrate low/mid/high- level features [50] and
 classifiers in an end-to-end multi- layer fashion, and the “levels” of
 features can be enriched by the number of stacked layers (depth).
\end_layout

\end_inset


\end_layout

\begin_layout Section
Materiais e métodos
\end_layout

\begin_layout Standard
Dentre as arquiteturas de redes neurais convolucionais profundas, o modelo
 que acreditamos ser o mais adequado são o de redes neurais residuais.
 Esse modelo, proposto inicialmente por He e colaboradores [He, 2015] facilitou
 o treinamento de redes convolucionais profundas, permitindo a construção
 de redes com até 1000 camadas de profundidade.
\end_layout

\begin_layout Standard
No modelo proposto por nós, a camada de entrada da rede utiliza a representação
 
\emph on
one hot encoding
\emph default
, onde cada aminoácido é representado por um vetor com 22 posições.
 Essas posições representam os 20 aminoácidos mais duas posições que representam
 a ausência de um aminoácido anterior ou posterior a sequência de resíduos
\emph on
.
 
\emph default
A utilização de códigos para a ausência de aminoácidos na posição tem como
 objetivo manter constante a somatória dos valores de entrada para as convoluçõe
s (Figura 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Codificação-da-sequência"

\end_inset

).
 
\end_layout

\begin_layout Standard
Com o objetivo de tornar mais didática a codificação de entrada, podemos
 comparar a entrada com uma imagem, o tipo de dado mais comum para redes
 convolucionais.
 As imagens coloridas (RGB) são representadas como 
\emph on
pixeis
\emph default
, onde cada 
\emph on
pixel
\emph default
 contém 3 valores, chamado de canais, que representam a intensidade das
 cores vermelho, verde e azul.
 O tamanho da imagem corresponde ao número de 
\emph on
pixeis
\emph default
 que ela tem de largura e altura.
 Assim, uma imagem colorida de tamanho 800x600, tem dimensões 3x600x800
 (CHW - 
\emph on
Channels, Height, Width
\emph default
).
\end_layout

\begin_layout Standard
Na codificação da proteína, o número de canais é igual a 22 e cada um representa
 a ocorrência de um aminoácido ou a ausência deles como mencionado anteriormente.
 Uma das dimensões representa a cadeia polipeptídica, essa dimensão tem
 tamanho 3000.
 Nessa dimensão, as primeiras 500 posições possuem o código de ausência
 de aminoácido anterior a proteína.
 A partir da posição 501, tem início os códigos que representam a sequência
 de resíduos da proteína.
 Após o término da sequência de resíduos, a matriz é preenchida com o código
 para ausência de aminoácidos posteriores a proteína.
 O tamanho foi pré-definido em 3000 para possibilitar que a cadeia polipeptídica
 mais longa em nosso conjunto de dados, a qual possui quase 2000 resíduos,
 tenha 500 códigos de ausência antes e depois.
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
Essa codificação permite testes com redes de até 250 blocos sem alteração
 nos dados.
\end_layout

\end_inset

 A outra dimensão da matriz é igual a 1, pois a cadeia polipeptídica é linear.
 Assim, a matriz de entrada possui dimensões 22x3000x1 (CHW).
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename figures/one_hot_encoding.pdf
	width 100line%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Codificação da sequência de aminoácidos da proteína no formato 
\emph on
one hot encoding.
 
\emph default
Os códigos '>' e '<' representam, respectivamente, a ausência de um aminoácido
 anterior e posterior a proteína.
 
\begin_inset CommandInset label
LatexCommand label
name "fig:Codificação-da-sequência"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
Através da codificação inicial da sequência no formato 
\emph on
one hot encoding
\emph default
 é possível apenas inferir se os aminoácidos são iguais ou diferentes.
 Consequentemente, um Aspartato diferirá igualmente de um Glutamato como
 de uma Fenilalanina, ou seja, essa codificação não contém similaridades
 estruturais e/ou físico-químicas.
\end_layout

\begin_layout Standard
Entretanto, ao invés de passarmos tais similaridades 
\emph on
a priori
\emph default
, optamos por utilizar como segunda camada da rede (primeira camada oculta)
 uma convolução 1x1, que amplia o número de canais de 22 para 
\begin_inset Formula $C$
\end_inset

, onde 
\begin_inset Formula $C=\{8,\,16,\,32\}$
\end_inset

.
 Essa camada tem como objetivo, aprender uma nova representação para os
 aminoácidos e assim, as similaridades entre eles (Figura 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Diagrama-de-uma"

\end_inset

).
\end_layout

\begin_layout Standard
A representação da segunda camada é mantida (residualmente) até a camada
 final, onde outra convolução 1x1 reduz o número de canais de 
\begin_inset Formula $C$
\end_inset

 para 3.
 Esses 3 canais finais estão relacionados aos 3 elementos de estrutura de
 secundária e, após a aplicação da função Softmax, representam a probabilidade
 de cada um dos 3 elementos, hélice, fita e coil, para cada resíduo da proteína.
\end_layout

\begin_layout Standard
É importante notar que nas camadas descritas até o momento, não há influência
 de resíduos vizinhos, dado que são utilizadas somente convoluções 1x1.
\end_layout

\begin_layout Standard
A influência da vizinhança é computada nos blocos da rede residual.
 O primeiro bloco, ou terceira camada, capta a saída da segunda camada e,
 após realizar seus cálculos, soma sua matriz de resultados a matriz de
 entrada.
 A matriz alterada pelo primeiro bloco, será a entrada do segundo bloco,
 e assim sucessivamente.
 Após as alterações feitas pelos blocos, a camada final, descrita anteriormente,
 fará a convolução 1x1 de 
\begin_inset Formula $C$
\end_inset

 para 3 canais.
 Como descrito, os resultados da segunda camada são mantidos, no entanto,
 eles são alterados pelos blocos, ou seja, eles são mantidos apenas residualment
e e por isso essa arquitetura de rede neural é chamada de rede residual.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename figures/residual_ss.pdf
	width 100line%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Diagrama da rede neural residual utilizada neste trabalho.
 A entrada da rede consiste na sequência de resíduos de uma proteína no
 formato 
\emph on
one-hot encoding.
 
\emph default
Para facilitar, apenas dois blocos estão
\emph on
 
\emph default
representados
\emph on
 
\emph default
e 
\begin_inset Formula $C$
\end_inset

=32 canais
\emph on
 
\emph default
(ou filtros)
\emph on
.
 
\emph default
No trabalho foram testadas redes com 4, 11 ou 21 blocos e 8, 16 ou 32 canais.

\emph on
 
\begin_inset CommandInset label
LatexCommand label
name "fig:Diagrama-de-uma"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
A proposta original para as redes residuais utilizam uma função de ativação
 não linear do tipo ReLU (
\emph on
Rectified Linear Unit
\emph default
) [?] ao longo do caminho residual [He, 2015].
 Dentro do bloco são utilizados, em sequência, uma convolução 3x3, uma camada
 de 
\emph on
batch normalization
\emph default
 [?], um ReLU, outra convolução 3x3 e outra camada de
\emph on
 batch normalization 
\emph default
(Figura 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Blocos"

\end_inset

 a).
 Posteriormente He e colaboradores [He, 2016] testaram diversas variações
 dos blocos e notaram que a alteração da ordem das camadas e a mudança do
 função de ativação do caminho residual para dentro do bloco melhoravam
 o desempenho da rede neural (Figura 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Blocos"

\end_inset

 b).
 
\end_layout

\begin_layout Standard
No entanto, alguns testes preliminares na fase de construção do nosso modelo
 demonstravam uma queda de desempenho com a utilização da camada de 
\emph on
batch normalization
\emph default
 (resultados não mostrados).
 Acreditamos que o motivo disso seja a influência das regiões com códigos
 que representam a ausência de resíduos possam ter no código dos aminoácidos
 e, uma vez que as cadeias polipeptídicas tem tamanhos diferentes para proteínas
 diferentes, essa influência possivelmente não é constante.
\end_layout

\begin_layout Standard
Por outro lado, a retirada da camada de 
\emph on
batch normalization
\emph default
 tornava as redes testadas mais suscetível ao sobreajuste (
\emph on
overfitting
\emph default
).
 A solução encontrada de regularização foi utilizar um camada de 
\emph on
dropout 
\emph default
[?] dentro do bloco, como utilizado por Zagoruyko e Komodakis [2017] nas
 
\emph on
Wide Residual Networks
\emph default
.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename figures/blocos_residuais.pdf
	width 100line%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Blocos 
\begin_inset CommandInset label
LatexCommand label
name "fig:Blocos"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Número de blocos e canais
\end_layout

\begin_layout Standard
O número de blocos define a profundidade da rede e, consequentemente, o
 tamanho da vizinhança, em número de resíduos, que irá influenciar na predição
 da estrutura secundária de cada resíduo.
\end_layout

\begin_layout Standard
A utilização de 4 blocos determina que a predição da estrutura secundária
 de cada resíduo utilizará a informação contida numa janela de 17 resíduos
 da proteína 
\begin_inset Formula $(4\times4+1)$
\end_inset

, sendo 8 resíduos anteriores e 8 resíduos posteriores.
 A rede com onze blocos utilizará a informação contida em uma janela de
 45 resíduos 
\begin_inset Formula $(11\times4+1)$
\end_inset

 e com 21 blocos, a informação em uma janela de 85 resíduos 
\begin_inset Formula $(21\times4+1)$
\end_inset

.
 Ou seja, a medida que o número de blocos aumenta, resíduos mais distantes
 na sequência proteica irão influenciar na predição da estrutura secundária
 local.
\end_layout

\begin_layout Standard
O número de canais tem relação com a capacidade de representar a informação
 local.
 Assim, os canais tem função similar ao número de neurônios da camada oculta
 no modelo de rede similar à de Holley e Karplus (Subseção 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Rede-neural-similar"

\end_inset

) e similar ao número de estados de transição do autômato celulares.
\end_layout

\begin_layout Subsection
Treinamento
\end_layout

\begin_layout Standard
Cada rede neural foi treinada por 1000 épocas utilizando o algoritmo de
 aprendizado Adam 
\begin_inset CommandInset citation
LatexCommand citet
key "kingma_adam:_2014"

\end_inset

 com taxa de aprendizado de 0.001.
\end_layout

\begin_layout Section
Resultados
\end_layout

\begin_layout Subsection
Análise do treinamento
\end_layout

\begin_layout Subsection
Distribuição da acurácia
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
grafico gerado com model_resnet_q_ss.py
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename figures/model_resnet_q_ss.pdf
	width 100line%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Distribuição do tamanho das estruturas secundárias preditas
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
grafico gerado com model_hk_len_ss.py
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename figures/model_resnet_len_ss.pdf
	width 100line%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Similaridade entre aminoácidos
\end_layout

\begin_layout Section
Conclusão
\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "cha:template"

\end_inset

Ibis redibis numquam peribis in bello.
\end_layout

\end_body
\end_document
